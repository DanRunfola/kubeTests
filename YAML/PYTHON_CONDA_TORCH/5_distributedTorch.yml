apiVersion: batch/v1
kind: Job
metadata:
  generateName: dist-
spec:
  parallelism: 2
  template:
    spec:
      restartPolicy: Never
      volumes:
        - name: home-volume
          persistentVolumeClaim:
            claimName: dsmillerrunfol-rwm
      containers:
        - name: distribute-torch-container
          image: "nvidia/samples:vectoradd-cuda11.2.1"
          resources:
            requests:
              memory: "16Gi"
              nvidia.com/gpu: 1
              cpu: "1"
              ephemeral-storage: "2Gi"
            limits:
              memory: "16Gi"
              nvidia.com/gpu: 1
              cpu: "1"
              ephemeral-storage: "2Gi"
          volumeMounts:
            - name: home-volume
              mountPath: /kube/home/
          env:
            - name: WORLD_SIZE
              value: "2"  # Total number of processes
            - name: JOB_IDENTIFIER
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          command: ["/bin/bash", "-c"]
          args:
            - |
              # Set the Miniconda path and initialize
              export MINICONDA_PATH="/kube/home/.envs/conda"
              export PATH="$MINICONDA_PATH/bin:$PATH"

              # Add the NVIDIA GPG key
              apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys A4B469963BF863CC
              apt-get update && apt-get install -y git --fix-missing

              # Activate the environment
              source activate torchEnv

              #Make sure our GPUs are loading
              python -c "import torch; gpus = torch.cuda.device_count(); print(f'Available GPUs: {gpus}'); [print(f'GPU {gpu}: {torch.cuda.get_device_name(gpu)}') for gpu in range(gpus)]"

              #Clone down our repo
              git clone https://www.github.com/DanRunfola/kubeTests
              cd kubeTests
              python dist_gpu.py
